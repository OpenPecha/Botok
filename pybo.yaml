tokenizer:
  trie_files:
    - &part 'particles.txt'
    - &ancient ancient.txt
    - &except exceptions.txt
    - &uncomp uncompound_lexicon.txt
    - &tsikchen tsikchen.txt
    - &oral0 oral_corpus_0.txt
    - &oral1 oral_corpus_1.txt
    - &oral2 oral_corpus_2.txt
    - &oral3 oral_corpus_3.txt
    - &record recordings_4.txt
    - &mgd mgd.txt
    - &verb verbs.txt
  skrt_files:
    - &skrt sanskrit.txt
  pos_files:
    - &tibdict Tibetan.DICT
  freq_files:
    - &freq_mgd mgd.txt
  Profile:
    pytib: [*ancient, *except, *uncomp, *tsikchen, *tibdict, *part]
    POS: [*ancient, *except, *uncomp, *tsikchen, *tibdict, *part]
    PP: [*part]
    GMD: [*ancient, *except, *uncomp, *tsikchen, *mgd, *verb, *tibdict, *skrt, *freq_mgd, *part]

pipeline:
  basic:
      pre: pre_basic
      tok: spaces
      proc: spaces_fulltext
      frm: plaintext
  pybo_raw_content:
      pre: pre_basic
      tok: pybo
      pybo_profile: GMD
      proc: pybo_raw_content
      frm: plaintext
  pybo_raw_lines:
      pre: pre_basic_lines
      tok: pybo
      pybo_profile: GMD
      proc: pybo_raw_content
      frm: plaintext
  syls:
      pre: pre_basic
      tok: syls
      proc: spaces_fulltext
      frm: plaintext
  pybo_raw_types:
      pre: pre_basic
      tok: pybo
      pybo_profile: GMD
      proc: pybo_raw_types
      frm: types